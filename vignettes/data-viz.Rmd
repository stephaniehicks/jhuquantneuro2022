---
title: "Data visualizations and the tidyverse"
description: >
  An introduction to data visualizations and the tidyverse
author: "Stephanie Hicks"
output:
  rmarkdown::html_document:
   highlight: pygments
   toc: true
   toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Data visualization and the tidyverse}
  %\VignetteEncoding[ut8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
```


# Overview


## Key resources

- Workshop material: [pkgdown website](https://stephaniehicks.com/jhuquantneuro2022)
- Code: [GitHub](https://github.com/stephaniehicks/jhuquantneuro2022)

## Learning objectives

-   Know difference between relative vs absolute paths
-   Use modern R packages (`readr`) for reading and writing data in R 
-   Understand the advantages of a `tibble` and `data.frame` data objects in R
-   Learn about the `dplyr` R package to manage data frames
-   Recognize the key verbs to manage data frames in `dplyr`
-   Use the "pipe" operator to combine verbs together
-   Be able to build up layers of graphics using `ggplot()`



# Reading and writing data 

Here, we introduce ays to read and write data (e.g. `.txt` and `.csv` files) using base R functions as well as more modern R packages, such as `readr`, which is typically [10x faster than base R](https://r4ds.had.co.nz/data-import.html#compared-to-base-r).


## Relative paths 

When you open up a `.Rproj` file, RStudio changes the path (location on your computer) to the `.Rproj`  location. 

After opening up a `.Rproj` file, you can test this by

```{r, eval=FALSE}
getwd()
```

When you open up someone else's R code or analysis, you might also see the

```{r, eval=FALSE}
setwd()
```

function being used which explicitly tells R to change the absolute path or absolute location of which directory to move into. 

For example, say I want to clone a GitHub repo from Roger, which has 100 R script files, and in every one of those files at the top is: 

```{r, eval=FALSE}
setwd("C:\Users\Roger\path\only\that\Roger\has")
```

The problem is, if I want to use his code, I will need to go and hand-edit every single one of those paths (`C:\Users\Roger\path\only\that\Roger\has`) to the path that I want to use on my computer  or wherever I saved the folder on my computer (e.g. `/Users/Stephanie/Documents/path/only/I/have`). 

1. This is an unsustainable practice. 
2. I can go in and manually edit the path, but this 
assumes I know how to set a working directory. Not 
everyone does. 

So instead of absolute paths: 

```{r, eval=FALSE}
setwd("/Users/jtleek/data")
setwd("~/Desktop/files/data")
setwd("C:\\Users\\Michelle\\Downloads")
```

A **better idea is to use relative paths with the [here](https://github.com/r-lib/here)
R package**. 

- It will recognize the top-level directory of a Git repo and supports building all paths 
relative to that. 
- For more on project-oriented workflow suggestions, read [this post](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) from Jenny Bryan.


### The `here` package

Let's try using the `here` package. 

```{r}
library(here)

here::here()
```
This function creates a path unique to my computer, but will also be unique to yours.

```{r}
list.files(here::here())
list.files(here("data"))
list.files(here("data", "team_standings.csv"))
```

Now we see that using the `here::here()` function is a _relative_ path (relative to the `.Rproj` file in our `jhuquantneuro2022` folder. 

Next, let's use the `here` package to read in some data with the `readr` package.

## The `readr` package

The `readr` package is recently developed by posit (formerly RStudio) to deal with reading in large flat files quickly. 
The package provides replacements for functions like `read.table()` and `read.csv()`. 
The analogous functions in `readr` are `read_table()` and `read_csv()`. 

These functions are often *much* faster than their base R analogues and provide a few other nice features such as progress meters.

For example, the package includes a variety of functions in the `read_*()` family that allow you to read in data from different formats of flat files. The following table gives a guide to several functions in the `read_*()` family. 

```{r echo = FALSE}
readr_functions <- data.frame(func = c("`read_csv()`",
                                       "`read_csv2()`",
                                       "`read_tsv()`",
                                       "`read_delim()`",
                                       "`read_fwf()`",
                                       "`read_log()`"),
                              file_type = c("Reads comma-separated file",
                                            "Reads semicolon-separated file",
                                            "Reads tab-separated file",
                                            "General function for reading delimited files",
                                            "Reads fixed width files",
                                            "Reads log files"))
knitr::kable(readr_functions, col.names = c("`readr` function", "Use"))
```

A typical call to `read_csv()` will look as follows.

```{r, echo=TRUE}
library(readr)
teams <- read_csv(here("data", "team_standings.csv"))
teams
```

# Data frames and tibbles

The **data frame** (or `data.frame`) is a **key data structure** in statistics and in R.

The basic structure of a data frame is that there is **one observation per row and each column represents a variable, a measure, feature, or characteristic of that observation**.

Given the importance of managing data frames, it is **important that we have good tools for dealing with them.**

For example, **operations** like filtering rows, re-ordering rows, and selecting columns, can often be tedious operations in R whose syntax is not very intuitive. The `dplyr` package in the `tidyverse` is designed to mitigate a lot of these problems and to provide a highly optimized set of routines specifically for dealing with data frames.

## Tibbles

Another type of data structure that we need to discuss is called the **tibble**! It's best to think of tibbles as an updated and stylish version of the `data.frame`.

Tibbles are what tidyverse packages work with most seamlessly. Now, that **does not mean tidyverse packages *require* tibbles**.

In fact, they still work with `data.frames`, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.

Before we go any further, tibbles *are* data frames, but they have some new bells and whistles to make your life easier.

## How tibbles differ from `data.frame`

There are a number of differences between tibbles and `data.frames`.

**Note**: To see a full vignette about tibbles and how they differ from data.frame, you will want to execute `vignette("tibble")` and read through that vignette.

We will summarize some of the most important points here:

-   **Input type remains unchanged** - `data.frame` is notorious for treating strings as factors; this will not happen with tibbles
-   **Variable names remain unchanged** - In base R, creating `data.frames` will remove spaces from names, converting them to periods or add "x" before numeric column names. Creating tibbles will not change variable (column) names.
-   **There are no `row.names()` for a tibble** - Tidy data requires that variables be stored in a consistent way, removing the need for row names.
-   **Tibbles print first ten rows and columns that fit on one screen** - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen.

## `as_tibble()`

Since many packages use the historical `data.frame` from base R, you will often find yourself in the situation that you have a `data.frame` and want to convert that `data.frame` to a `tibbl`e.

To do so, the `as_tibble()` function is exactly what you are looking for.

For the example, here we use a dataset (`chicago.rds`) containing air pollution and temperature data for the city of Chicago in the U.S.

The dataset is available in the `/data` repository. You can load the data into R using the `readRDS()` function.

```{r}
library(here)
chicago <- readRDS(here("data", "chicago.rds"))
```

You can see some basic characteristics of the dataset with the `dim()` and `str()` functions.

```{r}
dim(chicago)
str(chicago)
```

We see this data structure is a `data.frame` with 6940 observations and 8 variables.

To convert this `data.frame` to a tibble you would use the following:

```{r}
str(as_tibble(chicago))
```


# The `dplyr` Package

The `dplyr` package was developed by Posit (formely RStudio) and is **an optimized and distilled** version of the older `plyr` **package for data manipulation or wrangling**.

The `dplyr` package does not provide any "new" functionality to R per se, in the sense that everything `dplyr` does could already be done with base R, but it **greatly** simplifies existing functionality in R.

One important contribution of the `dplyr` package is that it **provides a "grammar" (in particular, verbs) for data manipulation and for operating on data frames**.

With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it **provides an abstraction for data manipulation that previously did not exist**.

Another useful contribution is that the `dplyr` functions are **very** fast, as many key operations are coded in C++.

## `dplyr` grammar

Some of the key "verbs" provided by the `dplyr` package are

-   `select()`: return a subset of the columns of a data frame, using a flexible notation

-   `filter()`: extract a subset of rows from a data frame based on logical conditions

-   `arrange()`: reorder rows of a data frame

-   `rename()`: rename variables in a data frame

-   `mutate()`: add new variables/columns or transform existing variables

-   `summarise()` / `summarize()`: generate summary statistics of different variables in the data frame, possibly within strata

-   `%>%`: the "pipe" operator is used to connect multiple verb actions together into a pipeline


**Note**: The `dplyr` package as a number of its own data types that it takes advantage of.

For example, there is a handy `print()` method that prevents you from printing a lot of data to the console. Most of the time, these additional data types are transparent to the user and do not need to be worried about.

# `dplyr` functions

All of the functions that we will discuss here will have a few common characteristics. In particular,

1.  The **first argument** is a data frame type object.

2.  The **subsequent arguments** describe what to do with the data frame specified in the first argument, and you can refer to columns in the data frame directly (without using the `$` operator, just use the column names).

3.  The **return result** of a function is a new data frame.

4.  Data frames must be **properly formatted** and annotated for this to all be useful. In particular, the data must be [tidy](http://www.jstatsoft.org/v59/i10/paper). In short, there should be one observation per row, and each column should represent a feature or characteristic of that observation.

## `dplyr` installation

The `dplyr` package is installed when you install and load the `tidyverse` meta-package.

```{r}
library(tidyverse)
```

You may get some warnings when the package is loaded because there are functions in the `dplyr` package that have the same name as functions in other packages. For now you can ignore the warnings.

## `select()`

We will continue to use the `chicago` dataset containing air pollution and temperature data.

```{r}
chicago <- as_tibble(chicago)
str(chicago)
```

The `select()` function can be used to **select columns of a data frame** that you want to focus on.

### Example

Suppose we wanted to take the first 3 columns only. There are a few ways to do this.

We could for example use numerical indices:

```{r}
names(chicago)[1:3]
```

But we can also use the names directly:

```{r}
subset <- select(chicago, city:dptp)
head(subset)
```


**Note**: The `:` normally cannot be used with names or strings, but inside the `select()` function you can use it to specify a range of variable names.

You can also **omit** variables using the `select()` function by using the negative sign. With `select()` you can do

```{r, eval=FALSE}
select(chicago, -(city:dptp))
```

which indicates that we should include every variable *except* the variables `city` through `dptp`. The equivalent code in base R would be

```{r,eval=FALSE}
i <- match("city", names(chicago))
j <- match("dptp", names(chicago))
head(chicago[, -(i:j)])
```

Not super intuitive, right?

The `select()` function also allows a special syntax that allows you to specify variable names based on patterns. So, for example, if you wanted to keep every variable that ends with a "2", we could do

```{r}
subset <- select(chicago, ends_with("2"))
str(subset)
```

Or if we wanted to keep every variable that starts with a "d", we could do

```{r}
subset <- select(chicago, starts_with("d"))
str(subset)
```

You can also use more general regular expressions if necessary. See the help page (`?select`) for more details.

## `filter()`

The `filter()` function is used to **extract subsets of rows** from a data frame. This function is similar to the existing `subset()` function in R but is quite a bit faster in my experience.

### Example

Suppose we wanted to extract the rows of the `chicago` data frame where the levels of PM2.5 are greater than 30 (which is a reasonably high level), we could do

```{r}
chic.f <- filter(chicago, pm25tmean2 > 30)
str(chic.f)
```


You can see that there are now only `r nrow(chic.f)` rows in the data frame and the distribution of the `pm25tmean2` values is.

```{r}
summary(chic.f$pm25tmean2)
```

We can place an arbitrarily complex logical sequence inside of `filter()`, so we could for example extract the rows where PM2.5 is greater than 30 *and* temperature is greater than 80 degrees Fahrenheit.

```{r}
chic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)
select(chic.f, date, tmpd, pm25tmean2)
```

Now there are only `r nrow(chic.f)` observations where both of those conditions are met.

Other logical operators you should be aware of include:

|  Operator |                  Meaning |                        Example |
|----------:|-------------------------:|-------------------------------:|
|      `==` |                   Equals |                 `city == chic` |
|      `!=` |           Does not equal |                 `city != chic` |
|       `>` |             Greater than |                  `tmpd > 32.0` |
|      `>=` | Greater than or equal to |                 `tmpd >- 32.0` |
|       `<` |                Less than |                  `tmpd < 32.0` |
|      `<=` |    Less than or equal to |                 `tmpd <= 32.0` |
|    `%in%` |              Included in | `city %in% c("chic", "bmore")` |
| `is.na()` |       Is a missing value |            `is.na(pm10tmean2)` |


**Note**: If you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the `!` operator to negate the whole statement.

A common use of this is to identify observations with non-missing data (e.g., `!(is.na(pm10tmean2))`).


## `arrange()`

The `arrange()` function is used to **reorder rows** of a data frame according to one of the variables/columns. Reordering rows of a data frame (while preserving corresponding order of other columns) is normally a pain to do in R. The `arrange()` function simplifies the process quite a bit.

Here we can order the rows of the data frame by date, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.

```{r}
chicago <- arrange(chicago, date)
```

We can now check the first few rows

```{r}
head(select(chicago, date, pm25tmean2), 3)
```

and the last few rows.

```{r}
tail(select(chicago, date, pm25tmean2), 3)
```

Columns can be arranged in descending order too by useing the special `desc()` operator.

```{r}
chicago <- arrange(chicago, desc(date))
```

Looking at the first three and last three rows shows the dates in descending order.

```{r}
head(select(chicago, date, pm25tmean2), 3)
tail(select(chicago, date, pm25tmean2), 3)
```

## `rename()`

**Renaming a variable** in a data frame in R is surprisingly hard to do! The `rename()` function is designed to make this process easier.

Here you can see the names of the first five variables in the `chicago` data frame.

```{r}
head(chicago[, 1:5], 3)
```

The `dptp` column is supposed to represent the dew point temperature and the `pm25tmean2` column provides the PM2.5 data.

However, these names are pretty obscure or awkward and probably be renamed to something more sensible.

```{r}
chicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)
head(chicago[, 1:5], 3)
```

The syntax inside the `rename()` function is to have the new name on the left-hand side of the `=` sign and the old name on the right-hand side.


## `mutate()`

The `mutate()` function exists to **compute transformations of variables** in a data frame. Often, you want to create new variables that are derived from existing variables and `mutate()` provides a clean interface for doing that.

For example, with air pollution data, we often want to *detrend* the data by subtracting the mean from the data.

-   That way we can look at whether a given day's air pollution level is higher than or less than average (as opposed to looking at its absolute level).

Here, we create a `pm25detrend` variable that subtracts the mean from the `pm25` variable.

```{r}
chicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))
head(chicago)
```

There is also the related `transmute()` function, which does the same thing as `mutate()` but then **drops all non-transformed variables**.

Here, we de-trend the PM10 and ozone (O3) variables.

```{r}
head(transmute(chicago, 
               pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),
               o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))
```

Note that there are only two columns in the transmuted data frame.

## `group_by()`

The `group_by()` function is used to **generate summary statistics** from the data frame within strata defined by a variable.

For example, in this air pollution dataset, you might want to know what the average annual level of PM2.5 is?

So the stratum is the year, and that is something we can derive from the `date` variable.

**In conjunction** with the `group_by()` function, we often use the `summarize()` function (or `summarise()` for some parts of the world).

**Note**: The **general operation** here is a combination of

1.  Splitting a data frame into separate pieces defined by a variable or group of variables (`group_by()`)
2.  Then, applying a summary function across those subsets (`summarize()`)



### Example

First, we can create a `year` variable using `as.POSIXlt()`.

```{r}
chicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)
```

Now we can create a separate data frame that splits the original data frame by year.

```{r}
years <- group_by(chicago, year)
```

Finally, we compute summary statistics for each year in the data frame with the `summarize()` function.

```{r}

summarize(years, 
          pm25 = mean(pm25, na.rm = TRUE), 
          o3 = max(o3tmean2, na.rm = TRUE), 
          no2 = median(no2tmean2, na.rm = TRUE))
```

`summarize()` returns a data frame with `year` as the first column, and then the annual summary statistics of `pm25`, `o3`, and `no2`.


## `%>%`

The pipeline operator `%>%` is very handy for **stringing together multiple `dplyr` functions in a sequence of operations**.

Notice above that every time we wanted to apply more than one function, the sequence gets buried in a sequence of nested function calls that is difficult to read, i.e.

```{r,eval=FALSE}
third(second(first(x)))
```

This **nesting is not a natural way** to think about a sequence of operations.

The `%>%` operator allows you to string operations in a left-to-right fashion, i.e.

```{r,eval=FALSE}
first(x) %>% second %>% third
```

### Example

Take the example that we just did in the last section, but here use the pipe operator in a single expression.

```{r}
mutate(chicago, year = as.POSIXlt(date)$year + 1900) %>%    
        group_by(year) %>% 
        summarize(years, pm25 = mean(pm25, na.rm = TRUE), 
          o3 = max(o3tmean2, na.rm = TRUE), 
          no2 = median(no2tmean2, na.rm = TRUE))
```



**Note**: In the above code, I pass the `chicago` data frame to the first call to `mutate()`, but then afterwards I do not have to pass the first argument to `group_by()` or `summarize()`.

- Once you travel down the pipeline with `%>%`, the first argument is taken to be the output of the previous element in the pipeline.


## `slice_*()`

The `slice_sample()` function of the `dplyr` package will allow you to see a **sample of random rows** in random order.

The number of rows to show is specified by the `n` argument.

-   This can be useful if you **do not want to print the entire tibble**, but you want to get a greater sense of the values.
-   This is a **good option for data analysis reports**, where printing the entire tibble would not be appropriate if the tibble is quite large.


### Example

```{r}
slice_sample(chicago, n = 10)
```

You can also use `slice_head()` or `slice_tail()` to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the `n` argument.

This will show the first 5 rows.

```{r}
slice_head(chicago, n = 5)
```

This will show the last 5 rows.

```{r}
slice_tail(chicago, n = 5)
```

# The ggplot2 Plotting System

In this section, we will get into a little more of the nitty gritty of **how `ggplot2` builds plots** and how you can customize various aspects of any plot.

## Basic components of a ggplot2 plot

A **`ggplot2` plot** consists of a number of **key components**. 

-   A **data frame**: stores all of the data that will be displayed on the plot

-   **aesthetic mappings**: describe how data are mapped to color, size, shape, location

-   **geoms**: geometric objects like points, lines, shapes

-   **facets**: describes how conditional/panel plots should be constructed

-   **stats**: statistical transformations like binning, quantiles, smoothing

-   **scales**: what scale an aesthetic map uses (example: left-handed = red, right-handed = blue)

-   **coordinate system**: describes the system in which the locations of the geoms will be drawn

It is **essential to organize your data into a data frame** before you start with `ggplot2` (and all the **appropriate metadata** so that your data frame is self-describing and your plots will be self-documenting).

When **building plots in `ggplot2`** (rather than using `qplot()`), the **"artist's palette" model may be the closest analogy**. 

Essentially, you start with some raw data, and then you **gradually add bits and pieces to it to create a plot**. 

Plots are built up in layers, with the typically ordering being

1.  Plot the data
2.  Overlay a summary
3.  Add metadata and annotation

For quick exploratory plots you may not get past step 1.

# Building up in layers

First, we can **create a `ggplot` object** that stores the dataset and the basic aesthetics for mapping the x- and y-coordinates for the plot. 

## `palmerpenguins` dataset

Here we will use the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) dataset. 
These data contain observations for 344 penguins. 
There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.

```{r, echo=FALSE, fig.cap="Palmer penguins", out.width='60%', fig.align='center'}
knitr::include_graphics("https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png")
```

\[**Source**: [Artwork by Allison Horst](https://github.com/allisonhorst/stats-illustrations)\]

```{r}
library(palmerpenguins)
```

```{r}
glimpse(penguins)
```

If we wanted to count the number of penguins for each of the three species, we can use the `count()` function in `dplyr`:

```{r}
penguins %>% 
  count(species)
```

For example, we see there are a total of 152 Adelie penguins in the `palmerpenguins` dataset. 

```{r}
g <- ggplot(penguins, aes(x = flipper_length_mm, 
                          y = bill_length_mm))
summary(g)
class(g)
```

You can see above that the object `g` contains the dataset `penguins` and the mappings.

Now, normally if you were to `print()` a `ggplot` object a plot would appear on the plot device, however, our object `g` actually does not contain enough information to make a plot yet.

```{r, fig.cap="Nothing to see here!"}
g <- penguins %>% 
      ggplot(aes(x = flipper_length_mm, 
                 y = bill_length_mm))
print(g)
```

## First plot with point layer

To make a scatter plot, we need add at least one **geom**, such as points. 

Here, we add the `geom_point()` function to create a traditional scatter plot.

```{r}
g <- penguins %>% 
      ggplot(aes(x = flipper_length_mm, 
                 y = bill_length_mm))
g + geom_point()
```

How does ggplot know what points to plot? In this case, it can grab them from the data frame `penguins` that served as the input into the `ggplot()` function.

## Adding more layers

### smooth

Because the data appear rather noisy, it might be better if we added a smoother on top of the points to see if there is a trend in the data with PM2.5.

```{r}
#| fig-cap: "Scatterplot with smoother"
#| message: false
g + 
  geom_point() + 
  geom_smooth()
```

The default smoother is a loess smoother, which is flexible and nonparametric but might be too flexible for our purposes. Perhaps we'd prefer a simple linear regression line to highlight any first order trends. We can do this by specifying `method = "lm"` to `geom_smooth()`.

```{r, message=FALSE}
g + 
  geom_point() + 
  geom_smooth(method = "lm")
```

Here, we can see there appears to be a increasing trend.

We can color the points by `species` and a smoother by adding a linear regression.

```{r, message=FALSE}
penguins %>% 
  ggplot(aes(x = flipper_length_mm, 
             y = bill_length_mm, 
             color = species)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


### facets

We can also stratify the scatter plot by another variable (e.g. `sex`) by **adding a `facet_grid()`** or `facet_wrap()` function. 

```{r, message=FALSE}
penguins %>% 
  filter(!is.na(sex)) %>% 
  ggplot(aes(x = flipper_length_mm, 
             y = bill_length_mm, 
             color = species)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_grid(.~sex)
```


### Changing the theme

The **default theme for `ggplot2` uses the gray background** with white grid lines. 

If you don't find this suitable, you can use the black and white theme by using the `theme_bw()` function.

The `theme_bw()` function also allows you to set the typeface for the plot, in case you don't want the default Helvetica. Here we change the typeface to Times.

For things that only make sense globally, use `theme()`, i.e. `theme(legend.position = "none")`. Two standard appearance themes are included

-   `theme_gray()`: The default theme (gray background)
-   `theme_bw()`: More stark/plain


```{r, message=FALSE}
g + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_bw(base_family = "Times")
```


### Modifying labels

There are a variety of **annotations** you can add to a plot, including **different kinds of labels**. 

- `xlab()` for x-axis labels
- `ylab()` for y-axis labels
- `ggtitle()` for specifying plot titles

`labs()` function is generic and can be used to modify multiple types of labels at once

:::

Here is an example of modifying the title and the `x` and `y` labels to make the plot a bit more informative.

```{r, message=FALSE, fig.cap="Modifying plot labels"}
g + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(title = "Palmer penguins", 
       x = "flipper length", 
       y = "bill length")
```


# Session Info

```{r}
sessionInfo()
```

